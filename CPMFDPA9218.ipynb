{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_1c80_0OFkC",
        "outputId": "0d95803b-aeab-4b47-ee5c-a6b6ebfd518c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.0+cpu.html\n",
            "Collecting pyg-lib\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcpu/pyg_lib-0.2.0%2Bpt113cpu-cp39-cp39-linux_x86_64.whl (626 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.9/626.9 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcpu/torch_scatter-2.1.1%2Bpt113cpu-cp39-cp39-linux_x86_64.whl (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.7/485.7 KB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcpu/torch_sparse-0.6.17%2Bpt113cpu-cp39-cp39-linux_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcpu/torch_cluster-1.6.1%2Bpt113cpu-cp39-cp39-linux_x86_64.whl (700 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m700.6/700.6 KB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt113cpu-cp39-cp39-linux_x86_64.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-geometric\n",
            "  Downloading torch_geometric-2.3.0.tar.gz (616 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.2/616.2 KB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (5.9.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch-geometric) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (1.26.15)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (1.1.1)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.3.0-py3-none-any.whl size=909897 sha256=c1e940f509c84c456b5055cc56c85a263228564531cf3570ff0d5ea6c196bf17\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/7d/6b/17150450b80b4a3656a84330e22709ccd8dc0f8f4773ba4133\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-spline-conv, torch-scatter, pyg-lib, torch-sparse, torch-cluster, torch-geometric\n",
            "Successfully installed pyg-lib-0.2.0+pt113cpu torch-cluster-1.6.1+pt113cpu torch-geometric-2.3.0 torch-scatter-2.1.1+pt113cpu torch-sparse-0.6.17+pt113cpu torch-spline-conv-1.2.2+pt113cpu\n"
          ]
        }
      ],
      "source": [
        "pip install pyg-lib torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.13.0+cpu.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import linalg\n",
        "from scipy.linalg import eigh\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "# import wandb\n",
        "from torch.nn.parameter import Parameter\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.sparse as sp\n",
        "from torch import optim\n",
        "import gc\n",
        "import os.path as osp\n",
        "import csv\n",
        "from typing import Optional\n",
        "from torch_geometric.typing import OptTensor\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.utils import get_laplacian\n",
        "from torch.autograd import Variable\n",
        "from torch_geometric.nn import GCNConv, GATConv,RGCNConv\n",
        "from torch_geometric.nn.inits import glorot, uniform\n",
        "from torch_geometric.utils import softmax\n",
        "from torch_sparse import SparseTensor, set_diag\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
        "from torch.nn import Sequential, Linear, ReLU, Dropout\n",
        "from scipy.special import comb\n",
        "from torch import Tensor\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "import copy\n",
        "import sklearn.metrics\n",
        "import time\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.sparse import coo_matrix\n",
        "from torch_geometric.nn.dense.linear import Linear as gLinear"
      ],
      "metadata": {
        "id": "Ob5Geq0XOL2Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO9mjn7UOL6Q",
        "outputId": "72c1c5ae-b7e5-43d6-8b0c-e82c93431283"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#random.seed(42)\n",
        "#np.random.seed(42)\n",
        "#torch.manual_seed(42)\n",
        "#torch.cuda.manual_seed(42)\n",
        "\n",
        "cudaid = \"cpu\"\n",
        "device = torch.device(cudaid)\n",
        "\n",
        "root = r\"/content/drive/MyDrive/data/GAPDA/\""
      ],
      "metadata": {
        "id": "mbTO-4nQOL9X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "piRNA = pd.read_csv(root + r\"piRNA.csv\", header = None).iloc[:,0].tolist()\n",
        "disease = pd.read_csv(root + r\"disease.csv\", header = None, sep = \"\\t\").iloc[:,0].tolist()\n",
        "disease_feature = pd.read_csv(root + r\"disease_feature.csv\", header = None, dtype=np.float32).to_numpy()\n",
        "piRNA_feature = pd.read_csv(root + r\"piRNA_feature.csv\", header = None, dtype=np.float32).to_numpy()\n",
        "label = pd.read_csv(root + r\"Label.csv\", header = None, sep = \"\\t\").iloc[:,0].tolist()\n",
        "FeatureList = pd.read_csv(root + r\"FeatureList.csv\", header = None, sep = \"\\t\", dtype=\"string\")"
      ],
      "metadata": {
        "id": "KwAwbvP3Or1x"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adj_normalization(w):\n",
        "    m = w.shape[0]\n",
        "    p = np.zeros([m, m])\n",
        "    for i in range(m):\n",
        "        for j in range(m):\n",
        "            if i == j:\n",
        "                p[i][j] = 1\n",
        "            else:                \n",
        "                p[i][j] = (w[i, j] - np.min(w[i, :])) / (np.max(w[i, :]) - np.min(w[i, :]))\n",
        "    return p"
      ],
      "metadata": {
        "id": "BDj3BaV3OMAy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disease_adj = adj_normalization(disease_feature)"
      ],
      "metadata": {
        "id": "02kVv6pkOMEq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ADJ = np.zeros((len(piRNA), len(disease)))\n",
        "edge_idx_dict = {}\n",
        "pos_1 = []\n",
        "pos_2 = []\n",
        "neg_1 = []\n",
        "neg_2 = []\n",
        "for i in range(len(label)):\n",
        "    for _,m in FeatureList.iloc[i,:].items():\n",
        "        m_list = m.split(\",\")\n",
        "        if len(m_list)>2:            \n",
        "            a = m_list[0]\n",
        "            b = \",\".join(m_list[1:],)\n",
        "            m_list = []\n",
        "            m_list.append(a)\n",
        "            m_list.append(b)\n",
        "        idx = piRNA.index(m_list[0])\n",
        "        idy = disease.index(m_list[1])\n",
        "        ADJ[idx, idy] = label[i]\n",
        "        if label[i] > 0:\n",
        "            pos_1.append(idx)\n",
        "            pos_2.append(idy)\n",
        "        else:\n",
        "            neg_1.append(idx)\n",
        "            neg_2.append(idy)\n",
        "edge_idx_dict['pos_edges'] = np.vstack((pos_1, pos_2))\n",
        "edge_idx_dict['neg_edges'] = np.vstack((neg_1, neg_2))\n",
        "\n",
        "print(sum(sum(ADJ)))\n",
        "print(ADJ.shape)\n",
        "print(edge_idx_dict['pos_edges'].shape)\n",
        "print(edge_idx_dict['neg_edges'].shape)\n",
        "print(np.median(ADJ.sum(1)))\n",
        "print(np.median(ADJ.sum(0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC2-7V-GOMIU",
        "outputId": "26bea68a-8f22-4486-a617-3457378d10f8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1212.0\n",
            "(501, 22)\n",
            "(2, 1212)\n",
            "(2, 1212)\n",
            "2.0\n",
            "13.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "piRNA_sim = np.zeros((piRNA_feature.shape[0], piRNA_feature.shape[0]))\n",
        "for i in range(piRNA_feature.shape[0]):\n",
        "    for j in range(piRNA_feature.shape[0]):\n",
        "        vec1 = piRNA_feature[i,:]\n",
        "        vec2 = piRNA_feature[j,:]\n",
        "        cos_sim = vec1.dot(vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "        piRNA_sim[i, j] = cos_sim"
      ],
      "metadata": {
        "id": "vjtCFRChOML7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GIP_kernel (Asso_RNA_Dis):\n",
        "    # the number of row\n",
        "    nc = Asso_RNA_Dis.shape[0]\n",
        "    #initate a matrix as result matrix\n",
        "    matrix = np.zeros((nc, nc))\n",
        "    # calculate the down part of GIP fmulate\n",
        "    r = getGosiR(Asso_RNA_Dis)\n",
        "    #calculate the result matrix\n",
        "    for i in range(nc):\n",
        "        for j in range(nc):\n",
        "            #calculate the up part of GIP formulate\n",
        "            temp_up = np.square(np.linalg.norm(Asso_RNA_Dis[i,:] - Asso_RNA_Dis[j,:]))\n",
        "            if r == 0:\n",
        "                matrix[i][j]=0\n",
        "            elif i==j:\n",
        "                matrix[i][j] = 1\n",
        "            else:\n",
        "                matrix[i][j] = np.e**(-temp_up/r)\n",
        "    return matrix\n",
        "def getGosiR (Asso_RNA_Dis):\n",
        "# calculate the r in GOsi Kerel\n",
        "    nc = Asso_RNA_Dis.shape[0]\n",
        "    summ = 0\n",
        "    for i in range(nc):\n",
        "        x_norm = np.linalg.norm(Asso_RNA_Dis[i,:])\n",
        "        x_norm = np.square(x_norm)\n",
        "        summ = summ + x_norm\n",
        "    r = summ / nc\n",
        "    return r\n",
        "\n",
        "def read_txt1(path):\n",
        "    with open(path, 'r', newline='') as txt_file:\n",
        "        md_data = []\n",
        "        reader = txt_file.readlines()\n",
        "        for row in reader:\n",
        "            line = row.split( )\n",
        "            row = []\n",
        "            for k in line:\n",
        "                row.append(float(k))\n",
        "            md_data.append(row)\n",
        "        md_data = np.array(md_data)\n",
        "        return md_data\n",
        "\n",
        "\n",
        "#W is the matrix which needs to be normalized\n",
        "def new_normalization (w):\n",
        "    m = w.shape[0]\n",
        "    p = np.zeros([m,m])\n",
        "    for i in range(m):\n",
        "        for j in range(m):\n",
        "            if i == j:\n",
        "                p[i][j] = 1/2\n",
        "            elif np.sum(w[i,:])-w[i,i]>0:\n",
        "                p[i][j] = w[i,j]/(2*(np.sum(w[i,:])-w[i,i]))\n",
        "    return p\n",
        "\n",
        "# get the KNN kernel, k is the number if first nearest neibors\n",
        "def KNN_kernel (S, k):\n",
        "    n = S.shape[0]\n",
        "    S_knn = np.zeros([n,n])\n",
        "    for i in range(n):\n",
        "        sort_index = np.argsort(S[i,:])\n",
        "        for j in sort_index[n-k:n]:\n",
        "            if np.sum(S[i,sort_index[n-k:n]])>0:\n",
        "                S_knn [i][j] = S[i][j] / (np.sum(S[i,sort_index[n-k:n]]))\n",
        "    return S_knn\n",
        "\n",
        "\n",
        "#updataing rules\n",
        "def MiRNA_updating (S1,S2,S3,S4, P1,P2,P3,P4):\n",
        "    it = 0\n",
        "    P = (P1+P2+P3+P4)/4\n",
        "    dif = 1\n",
        "    while dif>0.0000001:\n",
        "        it = it + 1\n",
        "        P111 =np.dot (np.dot(S1,(P2+P3+P4)/3),S1.T)\n",
        "        P111 = new_normalization(P111)\n",
        "        P222 =np.dot (np.dot(S2,(P1+P3+P4)/3),S2.T)\n",
        "        P222 = new_normalization(P222)\n",
        "        P333 = np.dot (np.dot(S3,(P1+P2+P4)/3),S3.T)\n",
        "        P333 = new_normalization(P333)\n",
        "        P444 = np.dot(np.dot(S4,(P1+P2+P3)/3),S4.T)\n",
        "        P444 = new_normalization(P444)\n",
        "        P1 = P111\n",
        "        P2 = P222\n",
        "        P3 = P333\n",
        "        P4 = P444\n",
        "        P_New = (P1+P2+P3+P4)/4\n",
        "        dif = np.linalg.norm(P_New-P)/np.linalg.norm(P)\n",
        "        P = P_New\n",
        "    print(\"Iter numb1\", it)\n",
        "    return P\n",
        "\n",
        "def disease_updating(S1,S2, P1,P2):\n",
        "    it = 0\n",
        "    P = (P1+P2)/2\n",
        "    dif = 1\n",
        "    while dif> 0.0000001:\n",
        "        it = it + 1\n",
        "        P111 =np.dot (np.dot(S1,P2),S1.T)\n",
        "        P111 = new_normalization(P111)\n",
        "        P222 =np.dot (np.dot(S2,P1),S2.T)\n",
        "        P222 = new_normalization(P222)\n",
        "        P1 = P111\n",
        "        P2 = P222\n",
        "        P_New = (P1+P2)/2\n",
        "        dif = np.linalg.norm(P_New-P)/np.linalg.norm(P)\n",
        "        P = P_New\n",
        "    print(\"Iter numb2\", it)\n",
        "    return P\n",
        "\n",
        "def get_syn_sim (A, k1, k2):\n",
        "    # disease_sim1 = read_txt1(\"./database/HMDD 2.0/disease_semantic_similarity.txt\")\n",
        "    # miRNA_sim1 = read_txt1(\"./database/HMDD 2.0/miRNA_functional_similarity.txt\")\n",
        "    # miRNA_sim2 = read_txt1 (\"./database/HMDD 2.0/miRNA_sequence_similarity.txt\")\n",
        "    # miRNA_sim3 = read_txt1(\"./database/HMDD 2.0/miRNA_semantic_similarity.txt\")\n",
        "\n",
        "    disease_sim1 = read_txt1(root + r\"disease_semantic_sim.txt\")\n",
        "    miRNA_sim1 = read_txt1(root + r\"miRNA_functional_sim.txt\")\n",
        "    miRNA_sim2 = read_txt1 (root + r\"miRNA_sequence_sim.txt\")\n",
        "    miRNA_sim3 = read_txt1(root + r\"miRNA_semantic_sim.txt\")\n",
        "\n",
        "    GIP_m_sim = GIP_kernel(A)\n",
        "    GIP_d_sim = GIP_kernel(A.T)\n",
        "    #miRNA_sim1 = GIP_m_sim\n",
        "    m1 = new_normalization(miRNA_sim1)\n",
        "    # m1 = miRNA_sim1\n",
        "    # m2 = miRNA_sim2\n",
        "    # m3 = miRNA_sim3\n",
        "    # m4 = GIP_m_sim\n",
        "    m2 = new_normalization(miRNA_sim2)\n",
        "    m3 = new_normalization(miRNA_sim3)\n",
        "    m4 = new_normalization(GIP_m_sim)\n",
        "    Sm_1 = KNN_kernel(miRNA_sim1, k1)\n",
        "    Sm_2 = KNN_kernel(miRNA_sim2, k1)\n",
        "    Sm_3 = KNN_kernel(miRNA_sim3, k1)\n",
        "    Sm_4 = KNN_kernel(GIP_m_sim, k1)\n",
        "    Pm = MiRNA_updating(Sm_1,Sm_2,Sm_3,Sm_4, m1, m2, m3,m4)\n",
        "    Pm_final = (Pm + Pm.T)/2\n",
        "    #np.save('m_sim_final.npy', Pm_final)\n",
        "    #np.save('m_sim_final.txt', Pm_final)\n",
        "    d1 = new_normalization(disease_sim1)\n",
        "    d2 = new_normalization(GIP_d_sim)\n",
        "    #d1 = disease_sim1\n",
        "    #d2 = GIP_d_sim\n",
        "    Sd_1 = KNN_kernel(disease_sim1, k2)\n",
        "    Sd_2 = KNN_kernel(GIP_d_sim, k2)\n",
        "    Pd = disease_updating(Sd_1,Sd_2, d1, d2)\n",
        "    Pd_final = (Pd+Pd.T)/2\n",
        "    #np.save('d_sim_final.npy', Pd_final)\n",
        "    #np.save('d_sim_final.txt', Pd_final)\n",
        "\n",
        "    return Pm_final, Pd_final\n",
        "\n",
        "def get_all_the_samples(A):\n",
        "    m,n = A.shape\n",
        "    pos = []\n",
        "    neg = []\n",
        "    for i in range(m):\n",
        "        for j in range(n):\n",
        "            if A[i,j] ==1:\n",
        "                pos.append([i,j,1])\n",
        "            else:\n",
        "                neg.append([i,j,0])\n",
        "    n = len(pos)\n",
        "    neg_new = random.sample(neg, n)\n",
        "    tep_samples = pos + neg_new\n",
        "    samples = random.sample(tep_samples, len(tep_samples))\n",
        "    samples = random.sample(samples, len(samples))\n",
        "    samples = np.array(samples)\n",
        "    return samples\n",
        "\n",
        "def get_all_the_samples1(A):\n",
        "    m,n = A.shape\n",
        "    pos = []\n",
        "    neg = []\n",
        "    for i in range(m):\n",
        "        for j in range(n):\n",
        "            if A[i,j] ==1:\n",
        "                pos.append([i,j,1])\n",
        "            else:\n",
        "                neg.append([i,j,0])\n",
        "    n = len(pos)\n",
        "    neg_new = random.sample(neg, n)\n",
        "    tep_samples = pos\n",
        "    samples = random.sample(tep_samples, len(tep_samples))\n",
        "    samples = random.sample(samples, len(samples))\n",
        "    samples = np.array(samples)\n",
        "    neg = np.array(neg)\n",
        "    return samples, neg\n",
        "\n",
        "def update_Adjacency_matrix (A, test_samples):\n",
        "    m = test_samples.shape[0]\n",
        "    A_tep = A.copy()\n",
        "    for i in range(m):\n",
        "        if test_samples[i,2] ==1:\n",
        "            A_tep [test_samples[i,0], test_samples[i,1]] = 0\n",
        "    return A_tep\n",
        "\n",
        "def set_digo_zero(sim, z):\n",
        "    sim_new = sim.copy()\n",
        "    n = sim.shape[0]\n",
        "    for i in range(n):\n",
        "        sim_new[i][i] = z\n",
        "    return sim_new\n",
        "\n",
        "#get the sparse similarity matrix, k is the first k nearest neibors\n",
        "\n",
        "def similarity_spare (A, k):\n",
        "    m = A.shape[0]\n",
        "    S = np.zeros([m,k])\n",
        "    for i in range(m):\n",
        "        tep = np.argsort(-A[i,:])\n",
        "        S[i,0:k] = tep[0:k]\n",
        "\n",
        "    W = np.zeros([m,m])\n",
        "    for i in range(m):\n",
        "        for j in range(m):\n",
        "            if i in S[j,:] and j in S[i,:]:\n",
        "                W[i,j] = 1\n",
        "            elif i in S[j,:] or j in S[i,:]:\n",
        "                W[i,j] = 0.5\n",
        "            else:\n",
        "                W[i,j] = 0\n",
        "    A = np.multiply(A, W)\n",
        "    return A\n"
      ],
      "metadata": {
        "id": "uCGVKT9ZOMP3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PMF(object):\n",
        "    def __init__(self, num_feat=16, epsilon=1, _lambda=0.1, momentum=0.8, maxepoch=200, num_batches=50, batch_size=10000):\n",
        "        self.num_feat = num_feat  # Number of latent features,\n",
        "        self.epsilon = epsilon  # learning rate,\n",
        "        self._lambda = _lambda  # L2 regularization,\n",
        "        self.momentum = momentum  # momentum of the gradient,\n",
        "        self.maxepoch = maxepoch  # Number of epoch before stop,\n",
        "        self.num_batches = num_batches  # Number of batches in each epoch (for SGD optimization),\n",
        "        self.batch_size = batch_size  # Number of training samples used in each batches (for SGD optimization)\n",
        "\n",
        "        self.w_Item = None  # Item feature vectors\n",
        "        self.w_User = None  # User feature vectors\n",
        "\n",
        "        self.rmse_train = []\n",
        "        self.rmse_test = []\n",
        "\n",
        "    # ***Fit the model with train_tuple and evaluate RMSE on both train and test data.  ***********#\n",
        "    # ***************** train_vec=TrainData, test_vec=TestData*************#\n",
        "    def fit(self, train_vec, test_vec):\n",
        "        #torch.manual_seed(42)\n",
        "        # mean subtraction\n",
        "        self.mean_inv = np.mean(train_vec[:, 2])  # 评分平均值\n",
        "\n",
        "        pairs_train = train_vec.shape[0]  # traindata 中条目数\n",
        "        pairs_test = test_vec.shape[0]  # testdata中条目数\n",
        "\n",
        "        # 1-p-i, 2-m-c\n",
        "        num_user = int(max(np.amax(train_vec[:, 0]), np.amax(test_vec[:, 0]))) + 1  # 第0列，user总数\n",
        "        num_item = int(max(np.amax(train_vec[:, 1]), np.amax(test_vec[:, 1]))) + 1  # 第1列，movie总数\n",
        "\n",
        "        incremental = False  # 增量\n",
        "        if ((not incremental) or (self.w_Item is None)):\n",
        "            # initialize\n",
        "            self.epoch = 0\n",
        "            self.w_Item = 0.1 * np.random.randn(num_item, self.num_feat)  # numpy.random.randn 电影 M x D 正态分布矩阵\n",
        "            self.w_User = 0.1 * np.random.randn(num_user, self.num_feat)  # numpy.random.randn 用户 N x D 正态分布矩阵\n",
        "\n",
        "            self.w_Item_inc = np.zeros((num_item, self.num_feat))  # 创建电影 M x D 0矩阵\n",
        "            self.w_User_inc = np.zeros((num_user, self.num_feat))  # 创建用户 N x D 0矩阵\n",
        "\n",
        "        while self.epoch < self.maxepoch:  # 检查迭代次数\n",
        "            self.epoch += 1\n",
        "\n",
        "            # Shuffle training truples\n",
        "            shuffled_order = np.arange(train_vec.shape[0])  # 根据记录数创建等差array\n",
        "            np.random.shuffle(shuffled_order)  # 用于将一个列表中的元素打乱\n",
        "\n",
        "            # Batch update\n",
        "            for batch in range(self.num_batches):  # 每次迭代要使用的数据量\n",
        "                # print \"epoch %d batch %d\" % (self.epoch, batch+1)\n",
        "\n",
        "                test = np.arange(self.batch_size * batch, self.batch_size * (batch + 1))\n",
        "                batch_idx = np.mod(test, shuffled_order.shape[0])  # 本次迭代要使用的索引下标\n",
        "\n",
        "                batch_UserID = np.array(train_vec[shuffled_order[batch_idx], 0], dtype='int32')\n",
        "                batch_ItemID = np.array(train_vec[shuffled_order[batch_idx], 1], dtype='int32')\n",
        "\n",
        "                # Compute Objective Function\n",
        "                pred_out = np.sum(np.multiply(self.w_User[batch_UserID, :],\n",
        "                                              self.w_Item[batch_ItemID, :]),\n",
        "                                  axis=1)  # mean_inv subtracted # np.multiply对应位置元素相乘\n",
        "\n",
        "                rawErr = pred_out - train_vec[shuffled_order[batch_idx], 2] + self.mean_inv\n",
        "\n",
        "                # Compute gradients\n",
        "                Ix_User = 2 * np.multiply(rawErr[:, np.newaxis], self.w_Item[batch_ItemID, :]) \\\n",
        "                       + self._lambda * self.w_User[batch_UserID, :]\n",
        "                Ix_Item = 2 * np.multiply(rawErr[:, np.newaxis], self.w_User[batch_UserID, :]) \\\n",
        "                       + self._lambda * (self.w_Item[batch_ItemID, :])  # np.newaxis :increase the dimension\n",
        "\n",
        "                dw_Item = np.zeros((num_item, self.num_feat))\n",
        "                dw_User = np.zeros((num_user, self.num_feat))\n",
        "\n",
        "                # loop to aggreate the gradients of the same element\n",
        "                for i in range(self.batch_size):\n",
        "                    dw_Item[batch_ItemID[i], :] += Ix_Item[i, :]\n",
        "                    dw_User[batch_UserID[i], :] += Ix_User[i, :]\n",
        "\n",
        "                # Update with momentum\n",
        "                self.w_Item_inc = self.momentum * self.w_Item_inc + self.epsilon * dw_Item / self.batch_size\n",
        "                self.w_User_inc = self.momentum * self.w_User_inc + self.epsilon * dw_User / self.batch_size\n",
        "\n",
        "                self.w_Item = self.w_Item - self.w_Item_inc\n",
        "                self.w_User = self.w_User - self.w_User_inc\n",
        "\n",
        "                # Compute Objective Function after\n",
        "                if batch == self.num_batches - 1:\n",
        "                    pred_out = np.sum(np.multiply(self.w_User[np.array(train_vec[:, 0], dtype='int32'), :],\n",
        "                                                  self.w_Item[np.array(train_vec[:, 1], dtype='int32'), :]),\n",
        "                                      axis=1)  # mean_inv subtracted\n",
        "                    rawErr = pred_out - train_vec[:, 2] + self.mean_inv\n",
        "                    obj = np.linalg.norm(rawErr) ** 2 \\\n",
        "                          + 0.5 * self._lambda * (np.linalg.norm(self.w_User) ** 2 + np.linalg.norm(self.w_Item) ** 2)\n",
        "\n",
        "                    self.rmse_train.append(np.sqrt(obj / pairs_train))\n",
        "\n",
        "                # Compute validation error\n",
        "                if batch == self.num_batches - 1:\n",
        "                    pred_out = np.sum(np.multiply(self.w_User[np.array(test_vec[:, 0], dtype='int32'), :],\n",
        "                                                  self.w_Item[np.array(test_vec[:, 1], dtype='int32'), :]),\n",
        "                                      axis=1)  # mean_inv subtracted\n",
        "                    rawErr = pred_out - test_vec[:, 2] + self.mean_inv\n",
        "                    self.rmse_test.append(np.linalg.norm(rawErr) / np.sqrt(pairs_test))\n",
        "\n",
        "                    # Print info\n",
        "                    #if batch == self.num_batches - 1:\n",
        "                    #    print('Training RMSE: %f, Test RMSE %f' % (self.rmse_train[-1], self.rmse_test[-1]))\n",
        "        return self.w_Item, self.w_User\n",
        "\n",
        "    def predict(self, invID):\n",
        "        return np.dot(self.w_Item, self.w_User[int(invID), :]) + self.mean_inv  # numpy.dot 点乘\n",
        "\n",
        "    # ****************Set parameters by providing a parameter dictionary.  ***********#\n",
        "    def set_params(self, parameters):\n",
        "        if isinstance(parameters, dict):\n",
        "            self.num_feat = parameters.get(\"num_feat\", 10)\n",
        "            self.epsilon = parameters.get(\"epsilon\", 1)\n",
        "            self._lambda = parameters.get(\"_lambda\", 0.1)\n",
        "            self.momentum = parameters.get(\"momentum\", 0.8)\n",
        "            self.maxepoch = parameters.get(\"maxepoch\", 20)\n",
        "            self.num_batches = parameters.get(\"num_batches\", 10)\n",
        "            self.batch_size = parameters.get(\"batch_size\", 1000)\n",
        "\n",
        "    def topK(self, test_vec, k=10):\n",
        "        inv_lst = np.unique(test_vec[:, 0])\n",
        "        pred = {}\n",
        "        for inv in inv_lst:\n",
        "            if pred.get(inv, None) is None:\n",
        "                pred[inv] = np.argsort(self.predict(inv))[-k:]  # numpy.argsort索引排序\n",
        "\n",
        "        intersection_cnt = {}\n",
        "        for i in range(test_vec.shape[0]):\n",
        "            if test_vec[i, 1] in pred[test_vec[i, 0]]:\n",
        "                intersection_cnt[test_vec[i, 0]] = intersection_cnt.get(test_vec[i, 0], 0) + 1\n",
        "        invPairs_cnt = np.bincount(np.array(test_vec[:, 0], dtype='int32'))\n",
        "\n",
        "        precision_acc = 0.0\n",
        "        recall_acc = 0.0\n",
        "        for inv in inv_lst:\n",
        "            precision_acc += intersection_cnt.get(inv, 0) / float(k)\n",
        "            recall_acc += intersection_cnt.get(inv, 0) / float(invPairs_cnt[int(inv)])\n",
        "\n",
        "        return precision_acc / len(inv_lst), recall_acc / len(inv_lst)"
      ],
      "metadata": {
        "id": "bqZZItzNOMTQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cheby(i,x):\n",
        "    if i==0:\n",
        "        return 1\n",
        "    elif i==1:\n",
        "        return 2*x\n",
        "    else:\n",
        "        T0=1\n",
        "        T1=2*x\n",
        "        for ii in range(2,i+1):\n",
        "            T2=2*x*T1-T0\n",
        "            T0,T1=T1,T2\n",
        "        return T2"
      ],
      "metadata": {
        "id": "g2CotAagOMW4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChebnetII_prop(MessagePassing):\n",
        "    def __init__(self, K, Init=False, bias=True, **kwargs):\n",
        "        super(ChebnetII_prop, self).__init__(aggr='add', **kwargs)\n",
        "        \n",
        "        self.K = K\n",
        "        self.temp = Parameter(torch.Tensor(self.K+1))\n",
        "        self.Init = Init\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.temp.data.fill_(1.0)\n",
        "\n",
        "        if self.Init:\n",
        "            for j in range(self.K+1):\n",
        "                x_j = math.cos((self.K-j+0.5)*math.pi/(self.K+1))\n",
        "                self.temp.data[j] = x_j**2\n",
        "        \n",
        "    def forward(self, x, edge_index,edge_weight=None):\n",
        "        coe_tmp = F.relu(self.temp)\n",
        "        coe = coe_tmp.clone()\n",
        "        \n",
        "        for i in range(self.K + 1):\n",
        "            coe[i] = coe_tmp[0]*cheby(i,math.cos((self.K + 0.5)*math.pi/(self.K + 1)))\n",
        "            for j in range(1,self.K + 1):\n",
        "                x_j = math.cos((self.K - j + 0.5)*math.pi/(self.K + 1))\n",
        "                coe[i] = coe[i] + coe_tmp[j]*cheby(i, x_j)\n",
        "            coe[i] = 2*coe[i]/(self.K + 1)\n",
        "\n",
        "\n",
        "        #L=I-D^(-0.5)AD^(-0.5)\n",
        "        edge_index1, norm1 = get_laplacian(edge_index, edge_weight,normalization='sym', dtype=x.dtype, num_nodes=x.size(self.node_dim))\n",
        "\n",
        "        #L_tilde=L-I\n",
        "        edge_index_tilde, norm_tilde = add_self_loops(edge_index1,norm1,fill_value=-1.0,num_nodes=x.size(self.node_dim))\n",
        "\n",
        "        Tx_0 = x\n",
        "        Tx_1 = self.propagate(edge_index_tilde,x=x,norm=norm_tilde,size=None)\n",
        "\n",
        "        out = coe[0]/2*Tx_0+coe[1]*Tx_1\n",
        "\n",
        "        for i in range(2,self.K+1):\n",
        "            Tx_2=self.propagate(edge_index_tilde,x=Tx_1,norm=norm_tilde,size=None)\n",
        "            Tx_2=2*Tx_2-Tx_0\n",
        "            out=out+coe[i]*Tx_2\n",
        "            Tx_0,Tx_1 = Tx_1, Tx_2\n",
        "        return out\n",
        "\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        return norm.view(-1, 1) * x_j\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}(K={}, temp={})'.format(self.__class__.__name__, self.K,\n",
        "                                          self.temp)\n",
        "\n"
      ],
      "metadata": {
        "id": "jI5nInJeOMaS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class VGAEModel2(nn.Module):\n",
        "    def __init__(self, num_features, hidden, out_dim, K, dropout = 0.5, device = 'cpu'):\n",
        "        super(VGAEModel2, self).__init__()        \n",
        "        self.num_features = num_features\n",
        "        self.hidden = hidden\n",
        "        self.out_dim = out_dim\n",
        "        self.K = K\n",
        "        self.dropout = dropout\n",
        "        self.device = device\n",
        "\n",
        "        self.lin1 = nn.Linear(num_features, hidden)\n",
        "        self.lin2 = nn.Linear(hidden, out_dim)\n",
        "\n",
        "        layers = [ChebnetII_prop(K), ChebnetII_prop(K), ChebnetII_prop(K//2)]\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        \n",
        "\n",
        "    def encoder(self, features, edge_index, adj):       \n",
        "        #x, edge_index\n",
        "\n",
        "        x = F.dropout(features, p = self.dropout, training=self.training)\n",
        "        x = self.lin1(x)\n",
        "        x = F.relu(x)        \n",
        "\n",
        "        x = F.dropout(x, p = self.dropout, training=self.training)\n",
        "        x = F.relu(self.lin2(x))\n",
        "        x = F.relu(self.layers[0](x, edge_index))\n",
        "        \n",
        "        #x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        #h = torch.relu(self.layers[0](x, edge_index))\n",
        "        #print(h.shape)\n",
        "        #h = t.relu(self.layers[1](h, edge_index, adj_tensor[i][edge_index[0], edge_index[1]]))\n",
        "        #self.mean = self.layers[1](h, edge_index)\n",
        "        #self.mean = t.relu(self.layers[1](h, edge_index))*0.01\n",
        "        #self.log_std = self.layers[2](h, edge_index)\n",
        "\n",
        "        self.mean = self.layers[1](x, edge_index)\n",
        "        self.log_std = self.layers[1](x, edge_index)\n",
        "\n",
        "        gaussian_noise = torch.randn(features.size(0), self.mean.shape[1]).to(device)\n",
        "        \n",
        "        #pl = t.poisson(self.mean)\n",
        "            \n",
        "        #print(\"-----------------------gaussian_noise.shape{}\".format(gaussian_noise.shape))\n",
        "        #print(\"-----------------------self.log_std.shape{}\".format(self.log_std.shape))\n",
        "            \n",
        "        sampled_z = self.mean + gaussian_noise * torch.exp(self.log_std).to(self.device)\n",
        "        return sampled_z\n",
        "        #return pl\n",
        "\n",
        "    def decoder(self, z):\n",
        "        adj_rec = torch.sigmoid(torch.mm(z, z.T))\n",
        "        return adj_rec\n",
        "\n",
        "    def forward(self, features, edge_index, adj):\n",
        "        z = self.encoder(features, edge_index, adj)\n",
        "        adj_rec = self.decoder(z)\n",
        "        return z,adj_rec, self.mean, self.log_std\n",
        "        #return z,adj_rec, self.mean"
      ],
      "metadata": {
        "id": "3e7FQfSfOMd8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_edge_index(matrix):\n",
        "    edge_index = [[], []]\n",
        "    for i in range(matrix.shape[0]):\n",
        "        for j in range(matrix.shape[1]):\n",
        "            if matrix[i, j] != 0:\n",
        "                edge_index[0].append(i)\n",
        "                edge_index[1].append(j)\n",
        "    return torch.LongTensor(edge_index)"
      ],
      "metadata": {
        "id": "jVG73o90OMhU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BuildModel3(nn.Module):  \n",
        " \n",
        "    def __init__(self, num_i, num_h1, num_h2, num_o, alpha = 0.5, dropout = 0.5):\n",
        "        super(BuildModel3,self).__init__()\n",
        "\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        self.linear11 = nn.Linear(num_i,num_h1)\n",
        "        self.relu11 = nn.ReLU()\n",
        "        self.linear12 = nn.Linear(num_i,num_h1)\n",
        "        self.relu12 = nn.ReLU()\n",
        "        \n",
        "        self.linear21 = nn.Linear(num_h1,num_h2) #2个隐层\n",
        "        self.relu21 = nn.ReLU()\n",
        "        self.linear22 = nn.Linear(num_h1,num_h2) #2个隐层\n",
        "        self.relu22 = nn.ReLU()\n",
        "        \n",
        "        self.linear31 = nn.Linear(num_h2,num_o)\n",
        "        self.linear32 = nn.Linear(num_h2,num_o)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.num_i = num_i\n",
        "  \n",
        "    def forward(self, x):\n",
        "        temp = x \n",
        "        x = temp[:,:32]\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear11(x)        \n",
        "        x = self.dropout(x)\n",
        "        x = self.relu11(x)\n",
        "        x = self.linear21(x)        \n",
        "        x = self.dropout(x)\n",
        "        x = self.relu21(x)\n",
        "        x1 = self.linear31(x)\n",
        "        \n",
        "        x = temp[:,32:]\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear12(x)        \n",
        "        x = self.dropout(x)\n",
        "        x = self.relu12(x)\n",
        "        x = self.linear22(x)        \n",
        "        x = self.dropout(x)\n",
        "        x = self.relu22(x)\n",
        "        x2 = self.linear32(x)        \n",
        "        \n",
        "        x = self.alpha*x1 + (1 - self.alpha)*x2\n",
        "        \n",
        "        return torch.sigmoid(x)\n"
      ],
      "metadata": {
        "id": "x-gd52bsOMk_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, adj_label_m, features_m, optimizer, epochs, norm):\n",
        "    #torch.manual_seed(42)\n",
        "    model.train()\n",
        "    edge_index = get_edge_index(adj_label_m)\n",
        "\n",
        "    adj_label_m_t = torch.Tensor(adj_label_m)\n",
        "    features_m_t = torch.Tensor(features_m)\n",
        "    pos_weight = []\n",
        "\n",
        "    for i in adj_label_m_t.reshape([-1]):\n",
        "        if i >= 0.5:\n",
        "            pos_weight.append(50)\n",
        "        else:\n",
        "            pos_weight.append(1)\n",
        "    regression_crit3 = nn.BCEWithLogitsLoss(weight = torch.Tensor(pos_weight))\n",
        "\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.zero_grad()\n",
        "        score, adj_rec, vgae_mean, vgae_std = model(features_m_t, edge_index, adj_label_m_t)        \n",
        "        loss = norm * regression_crit3(adj_rec.reshape([-1]), adj_label_m_t.reshape([-1]))      \n",
        "        \n",
        "        logits = adj_rec\n",
        "        kl_divergence = (0.5 / logits.shape[0]) * (1 + 2 * vgae_std - \n",
        "                        vgae_mean ** 2 - torch.exp(vgae_std) ** 2).sum(1).mean()\n",
        "        \n",
        "        loss -= kl_divergence       \n",
        "        \n",
        "        regression_crit3.weight = torch.Tensor(pos_weight)*torch.exp(-(2**0.5)*abs(adj_rec.reshape([-1]).detach() - \n",
        "                              adj_label_m_t.reshape([-1]).detach()))\n",
        "\n",
        "        loss = loss.requires_grad_()        \n",
        "        loss.backward()\n",
        "        optimizer.step()            \n",
        "        #print(\"0000\")\n",
        "        if epoch%100 == 0:\n",
        "            print(\"epoch : %d, loss:%.4f\" % (epoch, loss))\n",
        "    return score"
      ],
      "metadata": {
        "id": "kMPS8slcOMoV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Feature_synthesis_2(feature_MFm, feature_MFd, feature_m_Z, feature_d_Z, train_samples, test_samples):\n",
        "    n1, n2 = feature_MFm.shape[1], feature_MFd.shape[1] \n",
        "    n3, n4 = feature_m_Z.shape[1], feature_d_Z.shape[1]\n",
        "    Feature_dim = n1 + n2 + n3 + n4    \n",
        "\n",
        "    train_n = train_samples.shape[0]\n",
        "    train_feature = np.zeros([train_n, Feature_dim])\n",
        "    \n",
        "    train_label = np.zeros([train_n])\n",
        "    for i in range(train_n):\n",
        "        train_feature[i,0:n1] = feature_MFm[train_samples[i,0],:]\n",
        "        train_feature[i,n1 :(n1+n2)] = feature_MFd[train_samples[i,1], :]\n",
        "        train_feature[i,(n1+n2):(n1+n2+n3)] = feature_m_Z.detach()[train_samples[i,0],:]\n",
        "        train_feature[i, (n1+n2+n3):(n1+n2+n3+n4)] = feature_d_Z.detach()[train_samples[i,1],:]        \n",
        "        train_label[i] = train_samples[i,2]\n",
        "\n",
        "\n",
        "    # get the featrue vectors of test samples\n",
        "    test_N = test_samples.shape[0]\n",
        "    test_feature = np.zeros([test_N, Feature_dim])\n",
        "    \n",
        "    test_label = np.zeros(test_N)\n",
        "    for i in range(test_N):\n",
        "        test_feature[i,0:n1] = feature_MFm[test_samples[i,0],:]\n",
        "        test_feature[i,n1 :(n1+n2)] = feature_MFd[test_samples[i,1], :]\n",
        "        test_feature[i,(n1+n2):(n1+n2+n3)] = feature_m_Z.detach()[test_samples[i,0],:]\n",
        "        test_feature[i, (n1+n2+n3):(n1+n2+n3+n4)] = feature_d_Z.detach()[test_samples[i,1],:]        \n",
        "        test_label[i] = test_samples[i,2]\n",
        "    return train_feature, train_label, test_feature, test_label"
      ],
      "metadata": {
        "id": "xS5NBjP1OMrw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def skf_normalization(w):\n",
        "    row_sum = np.sum(w, axis=0)\n",
        "    p = (w / row_sum).T\n",
        "    return p\n",
        "\n",
        "\n",
        "def skf(A, seq_sim, str_sim, k1, k2):\n",
        "    disease_sim1 = str_sim\n",
        "    circRNA_sim1 = seq_sim\n",
        "\n",
        "    GIP_c_sim = GIP_kernel(A)\n",
        "    GIP_d_sim = GIP_kernel(A.T)\n",
        "    # miRNA_sim1 = GIP_m_sim\n",
        "    m1 = skf_normalization(circRNA_sim1)\n",
        "    m2 = skf_normalization(GIP_c_sim)\n",
        "\n",
        "    Sm_1 = KNN_kernel(circRNA_sim1, k1)\n",
        "    Sm_2 = KNN_kernel(GIP_c_sim, k1)\n",
        "    Pm = skf_updating(Sm_1, Sm_2, m1, m2, 0.1)\n",
        "    nei_weight1 = neighborhood_Com(Pm, k1)\n",
        "    Pm_final = Pm * nei_weight1\n",
        "\n",
        "    d1 = skf_normalization(disease_sim1)\n",
        "    d2 = skf_normalization(GIP_d_sim)\n",
        "\n",
        "    Sd_1 = KNN_kernel(disease_sim1, k2)\n",
        "    Sd_2 = KNN_kernel(GIP_d_sim, k2)\n",
        "    Pd = skf_updating(Sd_1, Sd_2, d1, d2, 0.1)\n",
        "    nei_weight2 = neighborhood_Com(Pd, k2)\n",
        "    Pd_final = Pd * nei_weight2\n",
        "\n",
        "    return Pm_final, Pd_final\n",
        "\n",
        "\n",
        "def skf_updating(S1, S2, P1, P2, alpha):\n",
        "    P = (P1 + P2) / 2\n",
        "    dif = 1\n",
        "    while dif > 0.0000001:\n",
        "        P111 = alpha * np.dot(np.dot(S1, P2), S1.T) + (1 - alpha) * P2\n",
        "        P111 = new_normalization(P111)\n",
        "        P222 = alpha * np.dot(np.dot(S2, P1), S2.T) + (1 - alpha) * P1\n",
        "        P222 = new_normalization(P222)\n",
        "        P1 = P111\n",
        "        P2 = P222\n",
        "        P_New = (P1 + P2) / 2\n",
        "        dif = np.linalg.norm(P_New - P) / np.linalg.norm(P)\n",
        "        P = P_New\n",
        "    return P\n",
        "\n",
        "\n",
        "def neighborhood_Com(sim, k):\n",
        "    weight = np.zeros(sim.shape)\n",
        "\n",
        "    for i in range(sim.shape[0]):\n",
        "        iu = sim[i, :]\n",
        "        iu_list = np.abs(np.sort(-iu))\n",
        "        iu_nearest_list_end = iu_list[k - 1]\n",
        "        for j in range(sim.shape[1]):\n",
        "            ju = sim[:, j]\n",
        "            ju_list = np.abs(np.sort(-ju))\n",
        "            ju_nearest_list_end = ju_list[k - 1]\n",
        "\n",
        "            if sim[i, j] >= iu_nearest_list_end and sim[i, j] >= ju_nearest_list_end:\n",
        "                weight[i, j] = 1\n",
        "                weight[j, i] = 1\n",
        "            elif sim[i, j] < iu_nearest_list_end and sim[i, j] < ju_nearest_list_end:\n",
        "                weight[i, j] = 0\n",
        "                weight[j, i] = 0\n",
        "            else:\n",
        "                weight[i, j] = 0.5\n",
        "                weight[j, i] = 0.5\n",
        "\n",
        "    return weight"
      ],
      "metadata": {
        "id": "A3v5B8OzOMzo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_matrix(matrix, k=20):\n",
        "    num = matrix.shape[0]\n",
        "    knn_graph = np.zeros(matrix.shape)\n",
        "    idx_sort = np.argsort(-(matrix - np.eye(num)), axis=1)\n",
        "    for i in range(num):\n",
        "        #将第i行最大的前k个值赋值给knn_graph(确保是对称矩阵)\n",
        "        knn_graph[i, idx_sort[i, :k + 1]] = matrix[i, idx_sort[i, :k + 1]]\n",
        "        knn_graph[idx_sort[i, :k + 1], i] = matrix[idx_sort[i, :k + 1], i]\n",
        "    return knn_graph + np.eye(num)\n"
      ],
      "metadata": {
        "id": "YAgOcO-pOM3d"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve,roc_curve, accuracy_score,f1_score,auc,precision_score,recall_score,matthews_corrcoef\n",
        "def cross_validation_experiment_3(samples, A, k_fold = 5, num_feat = 16,\n",
        "                  last_dim_m = 16, last_dim_d = 16, K_m = 16, K_d = 16, k1= 50, k2 = 5,\n",
        "                  dropout = 0.5, device = device, epochs = 1000, pos_weight = 50, norm = 0.5):\n",
        "    #A = miRNA_dis_matrix\n",
        "    kf = KFold(n_splits = k_fold, shuffle = True)    \n",
        "    metric = np.zeros((1, 7))\n",
        "    pre_matrix = np.zeros(A.shape)    \n",
        "    torch.manual_seed(42)\n",
        "    train_index, test_index = [],[]\n",
        "    for i, j in kf.split(samples):\n",
        "        train_index.append(i)\n",
        "        test_index.append(j)\n",
        "    auc_avg = 0\n",
        "    f1_avg = 0\n",
        "    aupr_avg = 0\n",
        "    mcc_avg = 0\n",
        "    acc_avg = 0\n",
        "    pre_avg = 0\n",
        "    rec_avg = 0\n",
        "    \n",
        "    for k in range(k_fold):\n",
        "        print(\"------this is %dth cross validation------\" % (k + 1))\n",
        "        train_samples = samples[train_index[k], :]\n",
        "        test_samples = samples[test_index[k], :]                \n",
        "        \n",
        "        pmf = None\n",
        "        pmf = PMF(num_feat = num_feat, maxepoch=200)\n",
        "        feature_MFd, feature_MFm = pmf.fit(train_samples, test_samples)        \n",
        "        #train_samples = samples[train_index[0], :]\n",
        "        #test_samples = samples[test_index[0], :]\n",
        "\n",
        "        new_A = update_Adjacency_matrix(A, test_samples)\n",
        "        #feature_MFm, feature_MFd = low_svd(new_A, meta_dim = 22)\n",
        "        features_m = new_A\n",
        "        features_d = new_A.transpose()\n",
        "        sim_m, sim_d = skf(new_A, piRNA_sim, disease_adj, k1, k2)\n",
        "        \n",
        "        m_adj = k_matrix(sim_m, k = 25)\n",
        "        d_adj = k_matrix(sim_d, k = 10)\n",
        "        \n",
        "        model = None\n",
        "        model = VGAEModel2(new_A.shape[1], 3*last_dim_m, last_dim_m, K = K_m, dropout = dropout, device = device)        \n",
        "        optimizer = None\n",
        "        optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 0.001)\n",
        "        feature_m_Z = train(model, m_adj, features_m, optimizer, epochs, norm)\n",
        "        model = None\n",
        "        model = VGAEModel2(new_A.shape[0], 3*last_dim_d, last_dim_d, K = K_d, dropout = dropout, device = device)        \n",
        "        optimizer = None\n",
        "        optimizer = optim.Adam(model.parameters(), lr = 0.0001, weight_decay = 0.001)\n",
        "        feature_d_Z = train(model, d_adj, features_d, optimizer, epochs, norm)\n",
        "        \n",
        "        train_feature, train_label, test_feature, test_label = Feature_synthesis_2(feature_MFm, feature_MFd,                                                                  \n",
        "                                                  feature_m_Z, feature_d_Z,\n",
        "                                                  train_samples, test_samples)       \n",
        "        \n",
        "        modelMLP = BuildModel3(32, 16, 8, 1, 0.9, dropout)        \n",
        "        train_label = torch.Tensor(train_label)\n",
        "        \n",
        "        regression_critMLP = nn.BCEWithLogitsLoss()        \n",
        "        optimizer_MLP = optim.Adam(modelMLP.parameters(), lr = 0.001, weight_decay = 0.0001)\n",
        "\n",
        "        train_feature = torch.Tensor(train_feature)\n",
        "\n",
        "        for i in range(150):\n",
        "            modelMLP.zero_grad()           \n",
        "            PRE = modelMLP(train_feature)       \n",
        "    \n",
        "            loss = regression_critMLP(PRE.view(1,-1), train_label.view(1,-1))           \n",
        "        \n",
        "            loss = loss.requires_grad_()           \n",
        "            loss.backward()\n",
        "            optimizer_MLP.step()\n",
        "            if i%50 == 0:\n",
        "                print(loss)\n",
        "\n",
        "        modelMLP.eval()\n",
        "        test_label = torch.Tensor(test_label)\n",
        "        test_feature = torch.Tensor(test_feature)\n",
        "        PRE_test = modelMLP(test_feature)\n",
        "        y_true = test_label.view(1,-1).numpy().flatten()  \n",
        "        y_score = PRE_test.view(1,-1).detach().numpy().flatten()\n",
        "        fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
        "        precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
        "        auroc=auc(fpr, tpr)\n",
        "        aupr=auc(recall, precision)\n",
        "        acc=accuracy_score(y_true, np.rint(y_score))\n",
        "        F1=f1_score(y_true, np.rint(y_score), average='macro')\n",
        "        Pre=precision_score(y_true, np.rint(y_score), average='macro')\n",
        "        Rec=recall_score(y_true, np.rint(y_score), average='macro')        \n",
        "        \n",
        "        Mcc=matthews_corrcoef(y_true,np.rint(y_score))        \n",
        "        print('AUC',auroc)\n",
        "        print('AUPR',aupr)\n",
        "        print('ACC',acc)\n",
        "        print('F1-score',F1)\n",
        "        print('Precision',Pre)\n",
        "        print('Recall',Rec)\n",
        "        print('MCC',Mcc)\n",
        "\n",
        "        auc_avg += auroc\n",
        "        aupr_avg += aupr\n",
        "        acc_avg += acc\n",
        "        f1_avg += F1\n",
        "        pre_avg += Pre\n",
        "        rec_avg += Rec\n",
        "        mcc_avg += Mcc\n",
        "\n",
        "    print(\"auc_avg: {}, aupr_avg: {}, acc_avg: {}, f1_avg: {}, pre_avg: {}, rec_avg: {}, mcc_avg: {}\".format(auc_avg/k_fold, aupr_avg/k_fold, acc_avg/k_fold, f1_avg/k_fold, pre_avg/k_fold, rec_avg/k_fold, mcc_avg/k_fold))\n",
        "    return PRE_test, test_label      \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gFH47-cpOM64"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = ADJ\n",
        "samples = get_all_the_samples(A)"
      ],
      "metadata": {
        "id": "zYBnvlQJOM-B"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PRE_test, test_label = cross_validation_experiment_3 (samples, A, k_fold = 5, num_feat = 16,\n",
        "                  last_dim_m = 16, last_dim_d = 16, K_m = 2, K_d = 2, k1= 51, k2 = 5,\n",
        "                  dropout = 0.01, device = device, epochs = 1000, pos_weight = 50, norm = 0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj-E-A2PONBp",
        "outputId": "f96d8c0d-ec05-4d3c-adce-7e96d2d6c4ea"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------this is 1th cross validation------\n",
            "epoch : 100, loss:0.3055\n",
            "epoch : 200, loss:0.3010\n",
            "epoch : 300, loss:0.2996\n",
            "epoch : 400, loss:0.2999\n",
            "epoch : 500, loss:0.3013\n",
            "epoch : 600, loss:0.2999\n",
            "epoch : 700, loss:0.2993\n",
            "epoch : 800, loss:0.2981\n",
            "epoch : 900, loss:0.2995\n",
            "epoch : 1000, loss:0.2993\n",
            "epoch : 100, loss:0.6716\n",
            "epoch : 200, loss:0.6500\n",
            "epoch : 300, loss:0.6583\n",
            "epoch : 400, loss:0.6530\n",
            "epoch : 500, loss:0.6325\n",
            "epoch : 600, loss:0.6396\n",
            "epoch : 700, loss:0.6197\n",
            "epoch : 800, loss:0.6260\n",
            "epoch : 900, loss:0.6328\n",
            "epoch : 1000, loss:0.6191\n",
            "tensor(0.7280, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "tensor(0.7161, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "tensor(0.6798, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "AUC 0.922506577373834\n",
            "AUPR 0.9045174027883576\n",
            "ACC 0.8144329896907216\n",
            "F1-score 0.8134615384615385\n",
            "Precision 0.8368504469850768\n",
            "Recall 0.8231540643045068\n",
            "MCC 0.6598623826405544\n",
            "------this is 2th cross validation------\n",
            "epoch : 100, loss:0.3421\n",
            "epoch : 200, loss:0.3104\n",
            "epoch : 300, loss:0.3047\n",
            "epoch : 400, loss:0.3018\n",
            "epoch : 500, loss:0.2987\n",
            "epoch : 600, loss:0.2993\n",
            "epoch : 700, loss:0.2997\n",
            "epoch : 800, loss:0.2998\n",
            "epoch : 900, loss:0.3001\n",
            "epoch : 1000, loss:0.2999\n",
            "epoch : 100, loss:0.6435\n",
            "epoch : 200, loss:0.6506\n",
            "epoch : 300, loss:0.6113\n",
            "epoch : 400, loss:0.6196\n",
            "epoch : 500, loss:0.6189\n",
            "epoch : 600, loss:0.6317\n",
            "epoch : 700, loss:0.6159\n",
            "epoch : 800, loss:0.6202\n",
            "epoch : 900, loss:0.6274\n",
            "epoch : 1000, loss:0.6408\n",
            "tensor(0.7339, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "tensor(0.7093, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "tensor(0.6453, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "AUC 0.913447782546495\n",
            "AUPR 0.8886125475750152\n",
            "ACC 0.8268041237113402\n",
            "F1-score 0.8250180393773837\n",
            "Precision 0.8520150223891376\n",
            "Recall 0.8320389672321002\n",
            "MCC 0.6837622524950436\n",
            "------this is 3th cross validation------\n",
            "epoch : 100, loss:0.3525\n",
            "epoch : 200, loss:0.3289\n",
            "epoch : 300, loss:0.3064\n",
            "epoch : 400, loss:0.3002\n",
            "epoch : 500, loss:0.3008\n",
            "epoch : 600, loss:0.2998\n",
            "epoch : 700, loss:0.3000\n",
            "epoch : 800, loss:0.3005\n",
            "epoch : 900, loss:0.3001\n",
            "epoch : 1000, loss:0.3000\n",
            "epoch : 100, loss:0.6769\n",
            "epoch : 200, loss:0.6451\n",
            "epoch : 300, loss:0.6256\n",
            "epoch : 400, loss:0.6275\n",
            "epoch : 500, loss:0.6309\n",
            "epoch : 600, loss:0.6402\n",
            "epoch : 700, loss:0.6423\n",
            "epoch : 800, loss:0.6331\n",
            "epoch : 900, loss:0.6183\n",
            "epoch : 1000, loss:0.6329\n",
            "tensor(0.7247, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "tensor(0.7046, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "tensor(0.6524, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "AUC 0.9228387711359848\n",
            "AUPR 0.8829477402525403\n",
            "ACC 0.8268041237113402\n",
            "F1-score 0.8250180393773837\n",
            "Precision 0.8456760331760331\n",
            "Recall 0.8289643792739767\n",
            "MCC 0.6744333968114105\n",
            "------this is 4th cross validation------\n",
            "epoch : 100, loss:0.3202\n",
            "epoch : 200, loss:0.3039\n",
            "epoch : 300, loss:0.3018\n",
            "epoch : 400, loss:0.3009\n",
            "epoch : 500, loss:0.3006\n",
            "epoch : 600, loss:0.3004\n",
            "epoch : 700, loss:0.3006\n",
            "epoch : 800, loss:0.2982\n",
            "epoch : 900, loss:0.2987\n",
            "epoch : 1000, loss:0.2994\n",
            "epoch : 100, loss:0.6681\n",
            "epoch : 200, loss:0.6472\n",
            "epoch : 300, loss:0.5901\n",
            "epoch : 400, loss:0.6490\n",
            "epoch : 500, loss:0.6489\n",
            "epoch : 600, loss:0.6300\n",
            "epoch : 700, loss:0.6229\n",
            "epoch : 800, loss:0.6278\n",
            "epoch : 900, loss:0.6174\n",
            "epoch : 1000, loss:0.6317\n",
            "tensor(0.7190, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "tensor(0.7064, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "tensor(0.6680, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "AUC 0.9285543852472986\n",
            "AUPR 0.9021632476377742\n",
            "ACC 0.7876288659793814\n",
            "F1-score 0.7761120448179273\n",
            "Precision 0.8329260345389378\n",
            "Recall 0.7786242628762314\n",
            "MCC 0.609134700918338\n",
            "------this is 5th cross validation------\n",
            "epoch : 100, loss:0.3061\n",
            "epoch : 200, loss:0.3039\n",
            "epoch : 300, loss:0.2992\n",
            "epoch : 400, loss:0.3003\n",
            "epoch : 500, loss:0.2999\n",
            "epoch : 600, loss:0.2995\n",
            "epoch : 700, loss:0.2994\n",
            "epoch : 800, loss:0.2996\n",
            "epoch : 900, loss:0.2991\n",
            "epoch : 1000, loss:0.2981\n",
            "epoch : 100, loss:0.7550\n",
            "epoch : 200, loss:0.6590\n",
            "epoch : 300, loss:0.6586\n",
            "epoch : 400, loss:0.6230\n",
            "epoch : 500, loss:0.6406\n",
            "epoch : 600, loss:0.6368\n",
            "epoch : 700, loss:0.6351\n",
            "epoch : 800, loss:0.6311\n",
            "epoch : 900, loss:0.6236\n",
            "epoch : 1000, loss:0.6427\n",
            "tensor(0.7301, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "tensor(0.7162, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "tensor(0.6748, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
            "AUC 0.9215504355445596\n",
            "AUPR 0.8948447099735226\n",
            "ACC 0.8636363636363636\n",
            "F1-score 0.8615080466148723\n",
            "Precision 0.867535115939736\n",
            "Recall 0.8591996976100889\n",
            "MCC 0.7266870096723681\n",
            "auc_avg: 0.9217795903696343, aupr_avg: 0.8946171296454419, acc_avg: 0.8238612933458294, f1_avg: 0.8202235417298211, pre_avg: 0.8470005306057843, rec_avg: 0.8243962742593809, mcc_avg: 0.670775948507543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "240436/1939"
      ],
      "metadata": {
        "id": "c6jW1p_9ONFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e196c313-d8c4-444b-9d7a-81954cd83381"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124.0"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "32+60"
      ],
      "metadata": {
        "id": "yLdYf_VGONIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43c33297-672b-42ad-e79f-c356ba8622d4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RXC5p_tbONLy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "0.5\n",
        "0.9107069987013399, aupr_avg: 0.9010285909670287, acc_avg: 0.8391113572463149, f1_avg: 0.8374643421649439, pre_avg: 0.8536387107358342, rec_avg: 0.8393448450560868, mcc_avg: 0.6928254313401777\n",
        "0.9  30\n",
        "0.913414992210841, aupr_avg: 0.8906125343785469, acc_avg: 0.8440598108545625, f1_avg: 0.8429268457860639, pre_avg: 0.8525443053545988, rec_avg: 0.8442178431163455, mcc_avg: 0.6966983739413687\n",
        "0.9  16_all\n",
        "0.9120714532002582, aupr_avg: 0.8959739679809801, acc_avg: 0.8106355968305359, f1_avg: 0.80659707617568, pre_avg: 0.8392021563954521, rec_avg: 0.8107835933162157, mcc_avg: 0.6493539014261125\n",
        "0.9  16_all dropout=0.6\n",
        "auc_avg: 0.9136362056152942, aupr_avg: 0.8901365312181572, acc_avg: 0.808587373264037, f1_avg: 0.8042759455295014, pre_avg: 0.8374207664158988, rec_avg: 0.808704727861774, mcc_avg: 0.6454623628470884\n",
        "\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "E0-GDAGG09IM",
        "outputId": "abfdbce5-a6ee-4ab8-ecf9-4f83d09b57e7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n0.5\\n0.9107069987013399, aupr_avg: 0.9010285909670287, acc_avg: 0.8391113572463149, f1_avg: 0.8374643421649439, pre_avg: 0.8536387107358342, rec_avg: 0.8393448450560868, mcc_avg: 0.6928254313401777\\n0.9  30\\n0.913414992210841, aupr_avg: 0.8906125343785469, acc_avg: 0.8440598108545625, f1_avg: 0.8429268457860639, pre_avg: 0.8525443053545988, rec_avg: 0.8442178431163455, mcc_avg: 0.6966983739413687\\n0.9  16_all\\n0.9120714532002582, aupr_avg: 0.8959739679809801, acc_avg: 0.8106355968305359, f1_avg: 0.80659707617568, pre_avg: 0.8392021563954521, rec_avg: 0.8107835933162157, mcc_avg: 0.6493539014261125\\n0.9  16_all dropout=0.6\\nauc_avg: 0.9136362056152942, aupr_avg: 0.8901365312181572, acc_avg: 0.808587373264037, f1_avg: 0.8042759455295014, pre_avg: 0.8374207664158988, rec_avg: 0.808704727861774, mcc_avg: 0.6454623628470884\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lDJcbQRh09MH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tveLWcZq09TT"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}